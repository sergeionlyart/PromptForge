LegalAlly / PromptForge

LegalAlly — минималистичный, полностью воспроизводимый пайплайн,
демонстрирующий, как автоматизировать итеративную разработку промптов
для аннотирования юридических документов при помощи LLM-моделей OpenAI.

⸻

1. Что это и зачем

Юридические документы объёмны, насыщены фактами, ссылками на нормы и нюансами формальной процедуры.
Чтобы быстро ориентироваться, адвокатам и корпоративным юристам нужны структурированные аннотации:
краткие, но точные резюме с перечислением фактов, сути спора, правовой мотивации и правовых последствий.

Проблема: даже сильные LLM иногда галлюцинируют, опускают детали или путают структуру.
Решение, заложенное в LegalAlly, — замкнутый цикл:
	1.	Сгенерировать аннотацию (LLM, шаг 1).
	2.	Жёстко раскритиковать её другой моделью (шаг 2).
	3.	Автоматически переписать исходный промпт на основе критики (шаг 3).
	4.	Снова применить улучшенный промпт и получить «чистовую» аннотацию (шаг 4).
	5.	Провести вторичное ревью для контроля качества (шаг 5).

Такой подход показывает, как с помощью самокоррекции и «подсказко-инженерии» (prompt engineering)
можно без ручного труда постепенно шлифовать инструкции до уровня «world-class summaries».

⸻

2. Как всё работает

graph TD;
    A[Markdown-документы] -->|o3-mini| B(Аннотация v1);
    B -->|o4-mini| C{Жёсткое ревью};
    C -->|o3-mini| D[Refined Prompt];
    D -->|o3-mini| E(Аннотация v2);
    E -->|o4-mini| F{Контрольное ревью};
    subgraph Экспорт
        B & C & D & E & F --> G[outputs/  + logs/]
    end

	1.	annotate_loop.py принимает 3 Markdown-файла (можно больше — см. ниже).
	2.	Каждый документ проходит пять фаз; все запросы/ответы логируются в logs/.
	3.	Результаты шагов складываются в outputs/:

Файл	Содержание
summary_*.md	Черновые аннотации
review_*.md	Первая критика
prompt_refined.txt	Новый, улучшенный промпт
revision_*.mdsummary2_*.md	Итоговые аннотации
review2_*.md	Финальный контроль


	4.	Все параметры (модели, уровни рассуждения, тексты подсказок) лежат в prompt.py
и подхватываются динамически — менять можно без правки кода.

⸻

3. Быстрый старт

3.1. Предварительные требования
	•	Python ≥ 3.9
	•	Аккаунт OpenAI + валидный OPENAI_API_KEY
	•	macOS / Linux / WSL — проверено; Windows тоже работает

3.2. Установка

git clone https://github.com/your-org/LegalAlly.git
cd LegalAlly/PromptForge
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt  # openai, typer, tqdm, etc.
export OPENAI_API_KEY="sk-..."   # или положите в .env

3.3. Запуск

python annotate_loop.py docs/act1.md docs/act2.md docs/act3.md

После выполнения смотрите папки:

PromptForge/
 ├─ outputs/
 │   ├─ summary_1.md       # черновик
 │   ├─ review_1.md        # критика
 │   ├─ summary2_1.md      # финальная аннотация
 │   └─ review2_1.md       # второе ревью
 └─ logs/
     └─ 20250629T101530Z_annotation.json


⸻

4. Настраиваем под себя

Хотите изменить	Что править
Структуру или критерии аннотации	prompt_start в prompt.py
Тон ревью (мягче/жёстче)	self_evaluation_prompt
Логику улучшения промпта	process_optimization_prompt
Язык (EN / RU / …)	Замена текстов промптов
Модели / тарифы	Словарь MODELS в annotate_loop.py
Уровень рассуждения	Аргумент effort в call_llm()

Совет: если нужен бесконечный цикл «оптимизируй → перепроверь»,
оберните annotate_loop.py в шедулер (cron, Airflow и т.д.) или сделайте
простой while True с порогом оценки качества.

⸻

5. Расширение списка документов

Скрипт принимает ровно три аргумента для наглядности.
Нужно больше? Замените сигнатуру main():

def main(docs: List[Path] = typer.Argument(...)):
    ...

И пройдитесь по местам, где сейчас жёстко указано doc_1, doc_2, doc_3.

⸻

6. Структура проекта

PromptForge/
 ├─ annotate_loop.py    # основной пайплайн
 ├─ prompt.py           # все промпты (едино место правки)
 ├─ docs/               # пример входных MD-файлов
 ├─ outputs/            # результат работы скрипта
 ├─ logs/               # JSON-трек запросов/ответов
 └─ requirements.txt    # зависимости (OpenAI SDK, Typer, Tqdm)


⸻

7. Что демонстрирует проект
	•	Практику prompt-engineering: как на основании критики автоматически генерировать улучшенную подсказку.
	•	LLM × LLM-self-critique: модель проверяет продукт другой модели.
	•	Минимальный, читаемый код: без LangChain и прочих тяжёлых фреймворков.
	•	Журналирование запросов: удобно анализировать стоимость и качество итераций.
	•	Рабочий шаблон для любых задач «генерируй → оцени → улучшай»
(техническая документация, маркетинговые сводки, клинические отчёты и т.п.).

⸻

8. Лицензия и дисклеймер

Код распространяется под лицензией MIT.
LLM-выводы не являются юридической консультацией.
Всегда проверяйте итоговые аннотации квалифицированным юристом,
особенно перед подачей в суд или регулятор.

⸻

9. Обратная связь и планы

Вопросы, идеи, pull-request’ы — welcome в Issues.
Дальнейшие шаги:
	•	Поддержка асинхронных вызовов + батчинг для удешевления запросов.
	•	Подключение моделей-open-source (Mistral, Claude-3) через совместимый интерфейс.
	•	Автоматический расчёт метрик (BLEU, Rouge, правовой F-score).

Enjoy prompt-crafting & let the LLM do the heavy lifting!